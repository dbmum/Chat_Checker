# -*- coding: utf-8 -*-
"""Working Senior Project Phase one.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H1ttW6osooYC0VoDPVgiavgrup4tvHUS
"""

import pandas as pd

data = pd.read_csv('/content/train_v3_drcat_01.csv')

data = data.dropna(subset=['text'])
data = data[data.RDizzl3_seven == True].reset_index(drop=True)

df = data

num_rows = 50

df_label_1 = df[df['label'] == 1].head(num_rows)

# Select 1000 rows with label 0
df_label_0 = df[df['label'] == 0].head(num_rows)

# Concatenate the two DataFrames
result_df = pd.concat([df_label_1, df_label_0])

# Reset index if needed
result_df = result_df.reset_index(drop=True)

result_df

import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer, AdamW
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming you have a labeled dataset with 'text' and 'label' columns
# Load and split your dataset
# For demonstration purposes, I'll use a random dataset with binary labels
# Replace this with your actual dataset loading code
texts = result_df.text.to_list()
labels = result_df.label.to_list()

# Split the dataset into training and testing sets
train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

# Tokenize the text data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')
test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors='pt')

train_labels = torch.tensor(train_labels)
test_labels = torch.tensor(test_labels)

# Create DataLoader for training and testing sets
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)

train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)

# Define the TransformerBinaryClassifier model
class TransformerBinaryClassifier(nn.Module):
    def __init__(self, pretrained_model_name='bert-base-uncased', hidden_size=768, num_classes=2):
        super(TransformerBinaryClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs['pooler_output']
        pooled_output = self.dropout(pooled_output)
        logits = self.fc(pooled_output)
        probabilities = self.softmax(logits)
        return probabilities

# Instantiate the model
model = TransformerBinaryClassifier()

# Define optimizer and loss function
optimizer = AdamW(model.parameters(), lr=1e-5)
criterion = nn.CrossEntropyLoss()

# Training loop
num_epochs = 3
for epoch in range(num_epochs):
    print(f'Epoch: {epoch}')
    model.train()
    for batch in train_dataloader:
        input_ids, attention_mask, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Evaluation on the test set
model.eval()
all_preds = []
all_labels = []
with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask)
        _, preds = torch.max(outputs, dim=1)
        all_preds.extend(preds.tolist())
        all_labels.extend(labels.tolist())

# Calculate accuracy
accuracy = accuracy_score(all_labels, all_preds)
print("Test Accuracy:", accuracy)

from google.colab import drive

# Mount Google Drive
drive.mount('/content/gdrive')

# Save the trained model to Google Drive
model_save_path = '/content/gdrive/My Drive/bert_binary_classifier.pth'
torch.save(model.state_dict(), model_save_path)

loaded_model = TransformerBinaryClassifier()
loaded_model.load_state_dict(torch.load('bert_binary_classifier.pth'))
loaded_model.eval()