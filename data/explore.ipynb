{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eplore.ipynb\n",
    "\n",
    "This file has some code for some potential transformations that you may want to do as well as some potential feature engineering that you can do for other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('articles.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = r'/(\\d{4})/(\\d{2})/(\\d{2})/'\n",
    "df[['year', 'month', 'day']] = df['URL'].str.extract(date_pattern)\n",
    "\n",
    "# Convert the extracted components to integers\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['month'] = df['month'].astype(int)\n",
    "df['day'] = df['day'].astype(int)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['generated_content'] = df['generated_content'].apply(lambda x:' '.join(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun all of the web scraping with updated paramaters to get the proper text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment, Tag\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extracting article title\n",
    "            article_title = soup.find(\"div\", class_=\"storytitle\").find(\"h1\").text.strip()\n",
    "\n",
    "            # Extracting article body\n",
    "            article_body = soup.find(\"div\", id=\"storytext\")\n",
    "\n",
    "            # Exclude divs with class 'credit-caption'\n",
    "            for div in article_body.find_all(\"div\", class_=\"credit-caption\"):\n",
    "                div.extract()\n",
    "\n",
    "            # Finding all paragraphs\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "            article_text = \"\"\n",
    "            for p in paragraphs:\n",
    "                # Extracting text from paragraph excluding links and nested tags\n",
    "                paragraph_text = ''.join([child.strip() if isinstance(child, str) else ' ' + child.text.strip() for child in p.contents if not isinstance(child, Comment)])\n",
    "                article_text += paragraph_text.strip() + \" \"\n",
    "            # Getting the first 200 words of the article\n",
    "            article_text = ' '.join(article_text.split()[:200])\n",
    "            return article_title, article_text\n",
    "        else:\n",
    "            print(f\"Failed to fetch URL: {url}, Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping URL: {url}, Error: {e}\")\n",
    "    return None, None\n",
    "\n",
    "df['Content'] = df['URL'].apply(lambda x:scrape_article(x)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_len'] = df['Content'].apply(lambda x:len(x))\n",
    "df['content_word'] = df['Content'].apply(lambda x:len(x.split()))\n",
    "df['generated_content_len'] = df['generated_content'].apply(lambda x:len(x))\n",
    "df['generated_content_word'] = df['generated_content'].apply(lambda x:len(x.split()))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare real vs generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Content'][0])\n",
    "print(df['generated_content'][0])\n",
    "print(df['URL'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Updated df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('./articles.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = pd.read_parquet('./articles.parquet')\n",
    "check_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
